{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtq6U3VbjbA4"
      },
      "outputs": [],
      "source": [
        "pip install segment_anything"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "id": "b3aT8OtkjfQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dlib"
      ],
      "metadata": {
        "id": "1IP6DWOBjhGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import dlib\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from ultralytics import YOLO  #\n",
        "\n",
        "# Load YOLO model\n",
        "yolo_model = YOLO('yolov8n.pt')\n"
      ],
      "metadata": {
        "id": "YDdd8yX0kADm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File path for the input video\n",
        "input_video_path = \"input_video.mp4\"  # Change according to video_name\n",
        "\n",
        "# Initialize video capture object\n",
        "video_capture = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Retrieve video properties\n",
        "frames_per_second = video_capture.get(cv2.CAP_PROP_FPS)  # Frames per second of the video\n",
        "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))  # Width of the video frames\n",
        "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Height of the video frames\n",
        "\n",
        "# File path for the output video\n",
        "output_video_path = \"output_video.avi\"\n",
        "\n",
        "# Initialize video writer object\n",
        "video_writer = cv2.VideoWriter(\n",
        "    output_video_path,                          # Output video file path\n",
        "    cv2.VideoWriter_fourcc(*'XVID'),           # Codec for the output video\n",
        "    frames_per_second,                         # Frame rate for the output video\n",
        "    (frame_width, frame_height)                # Frame dimensions for the output video\n",
        ")\n"
      ],
      "metadata": {
        "id": "6Ppa003HkgKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the SAM model checkpoint file\n",
        "sam_model_checkpoint_path = \"sam_vit_b_01ec64.pth\"  # Adjust path if using a different model or location\n",
        "\n",
        "# Type of SAM model being used\n",
        "sam_model_type = \"vit_b\"  # Specify the model type (e.g., \"vit_b\", \"vit_l\", etc.)\n",
        "\n",
        "# Set the computing device (GPU if available, otherwise CPU)\n",
        "compute_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the SAM model using the specified type and checkpoint\n",
        "sam_model = sam_model_registry[sam_model_type](checkpoint=sam_model_checkpoint_path).to(compute_device)\n",
        "\n",
        "# Initialize the SAM predictor\n",
        "sam_predictor = SamPredictor(sam_model)\n"
      ],
      "metadata": {
        "id": "bkCGeJV7luRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import dlib\n",
        "import cv2\n",
        "\n",
        "# Initialize the dlib correlation tracker\n",
        "object_tracker = dlib.correlation_tracker()\n",
        "\n",
        "# Read the first frame from the video capture\n",
        "success, initial_frame = video_capture.read()\n",
        "if not success:\n",
        "    print(\"Error: Unable to read the video file.\")\n",
        "    exit()\n",
        "\n",
        "# Specify the bounding box for object tracking\n",
        "# Coordinates for bounding box (x, y, width, height)\n",
        "# Uncomment the appropriate bounding box for the video being processed\n",
        "\n",
        "# Bounding box for Video 1\n",
        "bounding_box = (652, 207, 265, 497)\n",
        "\n",
        "# Bounding box for Video 2 or you can add respectively for the another video\n",
        "# bounding_box = (349, 353, 98, 78)\n",
        "\n",
        "# Optional: Uncomment to manually select the bounding box\n",
        "# bounding_box = cv2.selectROI(\"Select Object\", initial_frame, fromCenter=False)\n",
        "\n",
        "# Initialize the tracker with the selected bounding box\n",
        "object_tracker.start_track(\n",
        "    initial_frame,\n",
        "    dlib.rectangle(\n",
        "        int(bounding_box[0]),\n",
        "        int(bounding_box[1]),\n",
        "        int(bounding_box[0] + bounding_box[2]),\n",
        "        int(bounding_box[1] + bounding_box[3])\n",
        "    )\n",
        ")\n",
        "\n",
        "# Variables to manage frame count and periodic reset\n",
        "current_frame_count = 0\n",
        "reset_interval_frames = 30  # Interval to re-run YOLO detection\n",
        "\n",
        "# Variable to store the last computed mask (if applicable)\n",
        "previous_mask = None\n"
      ],
      "metadata": {
        "id": "L2ZCD43FnKDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process video frames until the end of the video\n",
        "while video_capture.isOpened():\n",
        "    # Read the next frame from the video\n",
        "    success, current_frame = video_capture.read()\n",
        "    if not success:\n",
        "        print(\"End of video reached.\")\n",
        "        break\n",
        "\n",
        "    # Periodically reset the tracker using YOLO object detection\n",
        "    if current_frame_count % reset_interval_frames == 0:\n",
        "        # Perform object detection using YOLO\n",
        "        yolo_results = yolo_model(current_frame)\n",
        "        detected_boxes = yolo_results[0].boxes\n",
        "\n",
        "        # If any detections are found, reset the tracker with the best detection\n",
        "        if len(detected_boxes) > 0:\n",
        "            # Extract the best detection's bounding box coordinates\n",
        "            best_box = detected_boxes[0].xyxy.cpu().numpy().astype(int)\n",
        "            x1, y1, x2, y2 = best_box[0]\n",
        "\n",
        "            # Reset the tracker with the new bounding box\n",
        "            object_tracker.start_track(current_frame, dlib.rectangle(x1, y1, x2, y2))\n",
        "            print(f\"Frame {current_frame_count}: Tracker reset using YOLO with bbox: {(x1, y1, x2 - x1, y2 - y1)}.\")\n",
        "\n",
        "    # Update the tracker with the current frame\n",
        "    object_tracker.update(current_frame)\n",
        "\n",
        "    # Get the updated bounding box from the tracker\n",
        "    tracked_position = object_tracker.get_position()\n",
        "    x1, y1, x2, y2 = int(tracked_position.left()), int(tracked_position.top()), int(tracked_position.right()), int(tracked_position.bottom())\n",
        "    tracked_bbox = (x1, y1, x2 - x1, y2 - y1)\n",
        "\n",
        "    # Use the SAM predictor to predict segmentation masks\n",
        "    sam_predictor.set_image(current_frame)\n",
        "    masks, scores, _ = sam_predictor.predict(\n",
        "        box=np.array([[tracked_bbox[0], tracked_bbox[1], tracked_bbox[0] + tracked_bbox[2], tracked_bbox[1] + tracked_bbox[3]]])\n",
        "    )\n",
        "\n",
        "    # Apply the segmentation mask to the current frame\n",
        "    if masks is not None and masks.size > 0:\n",
        "        # Convert the mask to a boolean array\n",
        "        previous_mask = masks[0].astype(bool)\n",
        "\n",
        "        # Convert the frame to grayscale for masked areas\n",
        "        grayscale_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
        "        grayscale_frame_3d = np.stack((grayscale_frame, grayscale_frame, grayscale_frame), axis=-1)\n",
        "\n",
        "        # Apply the segmentation mask: keep color where mask is True, grayscale where False\n",
        "        current_frame = np.where(previous_mask[..., None], current_frame, grayscale_frame_3d)\n",
        "\n",
        "    # Write the processed frame to the output video\n",
        "    video_writer.write(current_frame)\n",
        "    print(f\"Frame {current_frame_count} processed.\")\n",
        "\n",
        "    # Increment the frame counter\n",
        "    current_frame_count += 1\n",
        "\n",
        "# Release video capture and writer resources\n",
        "video_capture.release()\n",
        "video_writer.release()\n",
        "print(\"Video processing completed.\")\n"
      ],
      "metadata": {
        "id": "_F5eUi-Rn14F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}